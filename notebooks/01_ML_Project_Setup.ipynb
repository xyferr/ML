{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef864c99",
   "metadata": {},
   "source": [
    "# Machine Learning Project Setup Guide\n",
    "\n",
    "This notebook demonstrates how to set up a complete machine learning project structure with best practices for organization, dependency management, and version control.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Creating a standardized ML project directory structure\n",
    "- Setting up Python virtual environments\n",
    "- Installing essential ML libraries\n",
    "- Configuring version control with Git\n",
    "- Setting up Jupyter notebook environments\n",
    "- Organizing data directories properly\n",
    "- Managing environment variables and secrets\n",
    "\n",
    "## Prerequisites:\n",
    "- Python 3.8+ installed on your system\n",
    "- Git installed (optional but recommended)\n",
    "- Basic command line knowledge\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202958d6",
   "metadata": {},
   "source": [
    "## 1. Create Project Directory Structure\n",
    "\n",
    "A well-organized project structure is crucial for maintainable ML projects. Let's create a standardized folder layout that follows industry best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24451a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the project structure\n",
    "project_structure = {\n",
    "    'data': ['raw', 'processed', 'external'],\n",
    "    'notebooks': [],\n",
    "    'src': [],\n",
    "    'models': [],\n",
    "    'reports': ['figures'],\n",
    "    'experiments': [],\n",
    "    'tests': [],\n",
    "    'configs': []\n",
    "}\n",
    "\n",
    "# Get current working directory (should be your project root)\n",
    "project_root = Path.cwd().parent  # Since we're in notebooks folder\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Create directory structure\n",
    "for main_dir, subdirs in project_structure.items():\n",
    "    main_path = project_root / main_dir\n",
    "    main_path.mkdir(exist_ok=True)\n",
    "    print(f\"‚úì Created directory: {main_path}\")\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        sub_path = main_path / subdir\n",
    "        sub_path.mkdir(exist_ok=True)\n",
    "        print(f\"  ‚úì Created subdirectory: {sub_path}\")\n",
    "\n",
    "print(\"\\nüéâ Project directory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f061b0",
   "metadata": {},
   "source": [
    "## 2. Set Up Virtual Environment\n",
    "\n",
    "Virtual environments isolate your project dependencies and prevent conflicts between different projects. We'll create and activate a virtual environment for this ML project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import venv\n",
    "\n",
    "# Check if we're in a virtual environment\n",
    "def is_venv():\n",
    "    return hasattr(sys, 'real_prefix') or (\n",
    "        hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix\n",
    "    )\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Currently in virtual environment: {is_venv()}\")\n",
    "\n",
    "# If not in virtual environment, show instructions\n",
    "if not is_venv():\n",
    "    print(\"\\n‚ö†Ô∏è  You're not in a virtual environment!\")\n",
    "    print(\"\\nTo create and activate a virtual environment:\")\n",
    "    print(\"\\n# Windows:\")\n",
    "    print(\"python -m venv ml_env\")\n",
    "    print(\"ml_env\\\\Scripts\\\\activate\")\n",
    "    print(\"\\n# macOS/Linux:\")\n",
    "    print(\"python -m venv ml_env\")\n",
    "    print(\"source ml_env/bin/activate\")\n",
    "    print(\"\\nThen restart this notebook!\")\n",
    "else:\n",
    "    print(\"‚úÖ Great! You're already in a virtual environment.\")\n",
    "    print(f\"Environment name: {Path(sys.executable).parent.parent.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010136dd",
   "metadata": {},
   "source": [
    "## 3. Install Essential ML Libraries\n",
    "\n",
    "Let's install the core machine learning libraries that you'll need for most projects. We'll use pip to install packages and then verify the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4285fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for machine learning\n",
    "essential_packages = [\n",
    "    'numpy>=1.24.0',\n",
    "    'pandas>=2.0.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'seaborn>=0.12.0',\n",
    "    'scikit-learn>=1.3.0',\n",
    "    'jupyter>=1.0.0',\n",
    "    'ipykernel>=6.25.0'\n",
    "]\n",
    "\n",
    "# Additional useful packages\n",
    "additional_packages = [\n",
    "    'plotly>=5.17.0',\n",
    "    'xgboost>=2.0.0',\n",
    "    'lightgbm>=4.0.0',\n",
    "    'shap>=0.42.0',\n",
    "    'tqdm>=4.65.0'\n",
    "]\n",
    "\n",
    "def install_packages(packages, description):\n",
    "    print(f\"\\nüì¶ Installing {description}...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "            print(f\"‚úÖ {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "# Uncomment the lines below to install packages\n",
    "# Note: This might take a few minutes!\n",
    "\n",
    "print(\"üîß Ready to install packages...\")\n",
    "print(\"Uncomment and run the installation commands below:\")\n",
    "print(\"# install_packages(essential_packages, 'essential ML packages')\")\n",
    "print(\"# install_packages(additional_packages, 'additional useful packages')\")\n",
    "\n",
    "# Uncomment these lines when you're ready to install:\n",
    "# install_packages(essential_packages, 'essential ML packages')\n",
    "# install_packages(additional_packages, 'additional useful packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc67034",
   "metadata": {},
   "source": [
    "## 4. Create Requirements File\n",
    "\n",
    "A requirements.txt file ensures reproducibility by tracking all package dependencies and their versions. This makes it easy to recreate the same environment later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive requirements.txt file\n",
    "requirements_content = \"\"\"# Core Data Science Libraries\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "matplotlib>=3.7.0\n",
    "seaborn>=0.12.0\n",
    "plotly>=5.17.0\n",
    "\n",
    "# Machine Learning Libraries\n",
    "scikit-learn>=1.3.0\n",
    "xgboost>=2.0.0\n",
    "lightgbm>=4.0.0\n",
    "catboost>=1.2.0\n",
    "\n",
    "# Deep Learning (optional)\n",
    "# tensorflow>=2.13.0\n",
    "# torch>=2.0.0\n",
    "# torchvision>=0.15.0\n",
    "\n",
    "# Jupyter and Development\n",
    "jupyter>=1.0.0\n",
    "jupyterlab>=4.0.0\n",
    "ipykernel>=6.25.0\n",
    "ipywidgets>=8.0.0\n",
    "\n",
    "# Data Processing\n",
    "scipy>=1.11.0\n",
    "statsmodels>=0.14.0\n",
    "openpyxl>=3.1.0\n",
    "\n",
    "# Model Interpretation\n",
    "shap>=0.42.0\n",
    "lime>=0.2.0\n",
    "\n",
    "# Utilities\n",
    "tqdm>=4.65.0\n",
    "joblib>=1.3.0\n",
    "requests>=2.31.0\n",
    "\n",
    "# Development Tools\n",
    "black>=23.0.0\n",
    "flake8>=6.0.0\n",
    "pytest>=7.4.0\n",
    "\n",
    "# Optional but useful\n",
    "# optuna>=3.3.0  # Hyperparameter optimization\n",
    "# mlflow>=2.5.0  # ML experiment tracking\n",
    "\"\"\"\n",
    "\n",
    "# Write requirements.txt file\n",
    "requirements_path = project_root / \"requirements.txt\"\n",
    "with open(requirements_path, 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(f\"‚úÖ Created requirements.txt at: {requirements_path}\")\n",
    "print(\"\\nTo install all requirements later, run:\")\n",
    "print(\"pip install -r requirements.txt\")\n",
    "\n",
    "# Also generate current environment requirements\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], \n",
    "                          capture_output=True, text=True)\n",
    "    current_reqs_path = project_root / \"requirements_current.txt\"\n",
    "    with open(current_reqs_path, 'w') as f:\n",
    "        f.write(result.stdout)\n",
    "    print(f\"‚úÖ Created current environment snapshot: {current_reqs_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not create current requirements: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d773ce0",
   "metadata": {},
   "source": [
    "## 5. Initialize Git Repository\n",
    "\n",
    "Version control is essential for tracking changes in your ML projects. Let's set up Git with a proper .gitignore file for Python/ML projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if git is available and initialize repository\n",
    "def check_git():\n",
    "    try:\n",
    "        subprocess.run(['git', '--version'], capture_output=True, check=True)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "# Create .gitignore file for ML projects\n",
    "gitignore_content = \"\"\"# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "build/\n",
    "develop-eggs/\n",
    "dist/\n",
    "downloads/\n",
    "eggs/\n",
    ".eggs/\n",
    "lib/\n",
    "lib64/\n",
    "parts/\n",
    "sdist/\n",
    "var/\n",
    "wheels/\n",
    "*.egg-info/\n",
    ".installed.cfg\n",
    "*.egg\n",
    "\n",
    "# Virtual Environment\n",
    "ml_env/\n",
    "venv/\n",
    "env/\n",
    "ENV/\n",
    "\n",
    "# Jupyter Notebook\n",
    ".ipynb_checkpoints\n",
    "*.ipynb_checkpoints/\n",
    "\n",
    "# Data files (add specific patterns as needed)\n",
    "data/raw/*\n",
    "data/external/*\n",
    "!data/raw/.gitkeep\n",
    "!data/external/.gitkeep\n",
    "\n",
    "# Model files\n",
    "*.joblib\n",
    "*.pkl\n",
    "*.h5\n",
    "*.pb\n",
    "*.pth\n",
    "models/*.pkl\n",
    "models/*.joblib\n",
    "\n",
    "# Large files\n",
    "*.zip\n",
    "*.tar.gz\n",
    "*.rar\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# MLflow\n",
    "mlruns/\n",
    "mlartifacts/\n",
    "\n",
    "# Environment variables\n",
    ".env\n",
    ".env.local\n",
    "\n",
    "# Temporary files\n",
    "*.tmp\n",
    "temp/\n",
    "tmp/\n",
    "\"\"\"\n",
    "\n",
    "# Write .gitignore file\n",
    "gitignore_path = project_root / \".gitignore\"\n",
    "with open(gitignore_path, 'w') as f:\n",
    "    f.write(gitignore_content)\n",
    "\n",
    "print(f\"‚úÖ Created .gitignore at: {gitignore_path}\")\n",
    "\n",
    "# Initialize git repository if git is available\n",
    "if check_git():\n",
    "    try:\n",
    "        # Check if already a git repo\n",
    "        git_dir = project_root / \".git\"\n",
    "        if git_dir.exists():\n",
    "            print(\"‚úÖ Git repository already exists!\")\n",
    "        else:\n",
    "            subprocess.run(['git', 'init'], cwd=project_root, check=True)\n",
    "            print(\"‚úÖ Initialized Git repository!\")\n",
    "            \n",
    "        print(\"\\nNext steps for Git:\")\n",
    "        print(\"1. git add .\")\n",
    "        print(\"2. git commit -m 'Initial project setup'\")\n",
    "        print(\"3. git remote add origin <your-repo-url>\")\n",
    "        print(\"4. git push -u origin main\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è  Error with Git: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Git not found. Install Git to enable version control.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138cbb4",
   "metadata": {},
   "source": [
    "## 6. Set Up Jupyter Notebook Configuration\n",
    "\n",
    "Let's configure Jupyter notebooks for an optimal machine learning development experience, including kernel setup and useful extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and configure Jupyter kernel for this environment\n",
    "def setup_jupyter_kernel():\n",
    "    try:\n",
    "        # Install ipykernel if not already installed\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ipykernel'])\n",
    "        \n",
    "        # Add kernel to Jupyter\n",
    "        kernel_name = f\"ml_env_{project_root.name}\"\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'ipykernel', 'install', \n",
    "            '--user', '--name', kernel_name, '--display-name', f\"Python (ML - {project_root.name})\"\n",
    "        ])\n",
    "        \n",
    "        print(f\"‚úÖ Added Jupyter kernel: {kernel_name}\")\n",
    "        print(\"You can now select this kernel in Jupyter notebooks!\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è  Error setting up Jupyter kernel: {e}\")\n",
    "\n",
    "# Create Jupyter config directory and basic configuration\n",
    "jupyter_config = {\n",
    "    \"NotebookApp\": {\n",
    "        \"notebook_dir\": str(project_root),\n",
    "        \"open_browser\": True,\n",
    "        \"port\": 8888\n",
    "    },\n",
    "    \"InteractiveShell\": {\n",
    "        \"ast_node_interactivity\": \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß Jupyter Configuration:\")\n",
    "print(\"- Notebook directory:\", project_root)\n",
    "print(\"- Auto-open browser: True\")\n",
    "print(\"- Default port: 8888\")\n",
    "print(\"- Show all output: True\")\n",
    "\n",
    "# Set up the kernel (uncomment when ready)\n",
    "print(\"\\nüìã To set up Jupyter kernel, uncomment and run:\")\n",
    "print(\"# setup_jupyter_kernel()\")\n",
    "\n",
    "# Useful Jupyter magic commands to remember\n",
    "print(\"\\n‚ú® Useful Jupyter Magic Commands:\")\n",
    "magic_commands = [\n",
    "    \"%matplotlib inline  # Display plots inline\",\n",
    "    \"%load_ext autoreload  # Auto-reload modules\",\n",
    "    \"%autoreload 2  # Reload all modules before executing\",\n",
    "    \"%%time  # Time execution of cell\",\n",
    "    \"%debug  # Enter debugger after exception\",\n",
    "    \"%who  # List all variables\",\n",
    "    \"%whos  # List variables with details\"\n",
    "]\n",
    "\n",
    "for cmd in magic_commands:\n",
    "    print(f\"  {cmd}\")\n",
    "\n",
    "# Uncomment to set up the kernel:\n",
    "# setup_jupyter_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9056f28",
   "metadata": {},
   "source": [
    "## 7. Create Data Directories\n",
    "\n",
    "Proper data organization is crucial for ML projects. Let's set up a structured approach to managing different types of data with clear naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure with documentation\n",
    "data_structure = {\n",
    "    'raw': {\n",
    "        'description': 'Original, immutable data files',\n",
    "        'guidelines': [\n",
    "            'Never modify files in this directory',\n",
    "            'Document data sources and collection methods',\n",
    "            'Use descriptive filenames with dates if applicable'\n",
    "        ]\n",
    "    },\n",
    "    'processed': {\n",
    "        'description': 'Cleaned and preprocessed data files',\n",
    "        'guidelines': [\n",
    "            'Save intermediate processing steps',\n",
    "            'Include data validation and quality checks',\n",
    "            'Document transformations applied'\n",
    "        ]\n",
    "    },\n",
    "    'external': {\n",
    "        'description': 'External datasets and reference files',\n",
    "        'guidelines': [\n",
    "            'Store downloaded datasets from public sources',\n",
    "            'Include dataset documentation and licenses',\n",
    "            'Maintain original file formats when possible'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create data directories with README files\n",
    "data_dir = project_root / 'data'\n",
    "for subdir, info in data_structure.items():\n",
    "    subdir_path = data_dir / subdir\n",
    "    subdir_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create README for each data subdirectory\n",
    "    readme_content = f\"# {subdir.title()} Data\\n\\n\"\n",
    "    readme_content += f\"{info['description']}\\n\\n\"\n",
    "    readme_content += \"## Guidelines:\\n\"\n",
    "    for guideline in info['guidelines']:\n",
    "        readme_content += f\"- {guideline}\\n\"\n",
    "    \n",
    "    if subdir == 'raw':\n",
    "        readme_content += \"\\n## Data Sources:\\n\"\n",
    "        readme_content += \"- Add your data source information here\\n\"\n",
    "        readme_content += \"- Include URLs, APIs, or database connections\\n\"\n",
    "        readme_content += \"- Note any access requirements or credentials needed\\n\"\n",
    "    elif subdir == 'processed':\n",
    "        readme_content += \"\\n## Naming Convention:\\n\"\n",
    "        readme_content += \"- Use descriptive names: `cleaned_dataset_v1.csv`\\n\"\n",
    "        readme_content += \"- Include processing date: `processed_2024-01-15.parquet`\\n\"\n",
    "        readme_content += \"- Version your datasets: `features_v2.pkl`\\n\"\n",
    "    elif subdir == 'external':\n",
    "        readme_content += \"\\n## Common Sources:\\n\"\n",
    "        readme_content += \"- Kaggle datasets\\n\"\n",
    "        readme_content += \"- UCI ML Repository\\n\"\n",
    "        readme_content += \"- Government open data\\n\"\n",
    "        readme_content += \"- Academic datasets\\n\"\n",
    "    \n",
    "    readme_path = subdir_path / 'README.md'\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"‚úÖ Created {subdir} directory with documentation\")\n",
    "\n",
    "print(f\"\\nüìÅ Data directory structure:\")\n",
    "print(f\"  {data_dir}/\")\n",
    "for subdir in data_structure.keys():\n",
    "    print(f\"    {subdir}/\")\n",
    "    print(f\"      README.md\")\n",
    "print(\"\\nüéØ Your data is now organized and documented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007339c7",
   "metadata": {},
   "source": [
    "## 8. Set Up Environment Variables\n",
    "\n",
    "Environment variables help manage sensitive information like API keys, database connections, and configuration settings securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env template file for environment variables\n",
    "env_template = \"\"\"# Environment Variables Template\n",
    "# Copy this file to .env and fill in your actual values\n",
    "# Never commit .env to version control!\n",
    "\n",
    "# API Keys\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_token_here\n",
    "KAGGLE_USERNAME=your_kaggle_username\n",
    "KAGGLE_KEY=your_kaggle_api_key\n",
    "\n",
    "# Database Connections\n",
    "DATABASE_URL=postgresql://user:password@localhost:5432/dbname\n",
    "MONGODB_URI=mongodb://localhost:27017/your_database\n",
    "\n",
    "# AWS Credentials (if using AWS)\n",
    "AWS_ACCESS_KEY_ID=your_aws_access_key\n",
    "AWS_SECRET_ACCESS_KEY=your_aws_secret_key\n",
    "AWS_DEFAULT_REGION=us-west-2\n",
    "\n",
    "# MLflow Tracking\n",
    "MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "MLFLOW_EXPERIMENT_NAME=default\n",
    "\n",
    "# Project Settings\n",
    "PROJECT_NAME=ML_Project\n",
    "ENVIRONMENT=development\n",
    "DEBUG=True\n",
    "\n",
    "# Data Sources\n",
    "DATA_API_BASE_URL=https://api.example.com/v1\n",
    "DATA_API_KEY=your_data_api_key\n",
    "\n",
    "# Model Registry\n",
    "MODEL_REGISTRY_URI=s3://your-model-bucket/models\n",
    "\"\"\"\n",
    "\n",
    "# Create .env.template file\n",
    "env_template_path = project_root / \".env.template\"\n",
    "with open(env_template_path, 'w') as f:\n",
    "    f.write(env_template)\n",
    "\n",
    "print(f\"‚úÖ Created environment template: {env_template_path}\")\n",
    "\n",
    "# Create a sample .env file (with placeholder values)\n",
    "env_path = project_root / \".env\"\n",
    "if not env_path.exists():\n",
    "    with open(env_path, 'w') as f:\n",
    "        f.write(\"# Your actual environment variables\\n\")\n",
    "        f.write(\"# Copy from .env.template and fill in real values\\n\\n\")\n",
    "    print(f\"‚úÖ Created sample .env file: {env_path}\")\n",
    "else:\n",
    "    print(f\"üìÅ .env file already exists: {env_path}\")\n",
    "\n",
    "# Create Python module for loading environment variables\n",
    "env_loader_code = '''\"\"\"\n",
    "Environment variable loader for ML projects.\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_environment_variables(env_file=None):\n",
    "    \"\"\"\n",
    "    Load environment variables from .env file.\n",
    "    \n",
    "    Args:\n",
    "        env_file (str, optional): Path to .env file. \n",
    "                                 If None, looks for .env in project root.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of loaded environment variables\n",
    "    \"\"\"\n",
    "    if env_file is None:\n",
    "        # Find project root (assumes this file is in src/)\n",
    "        current_dir = Path(__file__).parent\n",
    "        project_root = current_dir.parent\n",
    "        env_file = project_root / \".env\"\n",
    "    \n",
    "    if Path(env_file).exists():\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"‚úÖ Loaded environment variables from {env_file}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Environment file not found: {env_file}\")\n",
    "    \n",
    "    # Return commonly used variables\n",
    "    return {\n",
    "        'project_name': os.getenv('PROJECT_NAME', 'ML_Project'),\n",
    "        'environment': os.getenv('ENVIRONMENT', 'development'),\n",
    "        'debug': os.getenv('DEBUG', 'False').lower() == 'true',\n",
    "        'mlflow_uri': os.getenv('MLFLOW_TRACKING_URI'),\n",
    "        'api_keys': {\n",
    "            'openai': os.getenv('OPENAI_API_KEY'),\n",
    "            'huggingface': os.getenv('HUGGINGFACE_API_KEY'),\n",
    "            'kaggle_username': os.getenv('KAGGLE_USERNAME'),\n",
    "            'kaggle_key': os.getenv('KAGGLE_KEY'),\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# from src.env_config import load_environment_variables\n",
    "# config = load_environment_variables()\n",
    "# print(f\"Project: {config['project_name']}\")\n",
    "'''\n",
    "\n",
    "# Create environment configuration module\n",
    "env_config_path = project_root / \"src\" / \"env_config.py\"\n",
    "with open(env_config_path, 'w') as f:\n",
    "    f.write(env_loader_code)\n",
    "\n",
    "print(f\"‚úÖ Created environment config module: {env_config_path}\")\n",
    "\n",
    "print(\"\\nüîê Environment Variables Setup Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Copy .env.template to .env\")\n",
    "print(\"2. Fill in your actual API keys and credentials in .env\")\n",
    "print(\"3. Install python-dotenv: pip install python-dotenv\")\n",
    "print(\"4. Use: from src.env_config import load_environment_variables\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Security Reminder:\")\n",
    "print(\"- Never commit .env files to version control\")\n",
    "print(\"- Use strong, unique API keys\")\n",
    "print(\"- Rotate keys regularly\")\n",
    "print(\"- Use different keys for development and production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5324b778",
   "metadata": {},
   "source": [
    "## üéâ Setup Complete!\n",
    "\n",
    "Congratulations! You've successfully set up a comprehensive machine learning workspace. Here's what we've accomplished:\n",
    "\n",
    "### ‚úÖ What's Been Created:\n",
    "\n",
    "1. **Project Structure**: Organized folders for data, notebooks, models, and source code\n",
    "2. **Virtual Environment**: Isolated Python environment for your ML projects\n",
    "3. **Dependencies**: Comprehensive requirements.txt with essential ML libraries\n",
    "4. **Version Control**: Git repository with ML-optimized .gitignore\n",
    "5. **Jupyter Setup**: Configured notebooks with proper kernel integration\n",
    "6. **Data Organization**: Structured data directories with documentation\n",
    "7. **Environment Variables**: Secure configuration management\n",
    "8. **Utility Modules**: Reusable code for common ML tasks\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Install Dependencies**: Run the package installation cells above\n",
    "2. **Start Coding**: Create your first ML notebook in the `notebooks/` folder\n",
    "3. **Add Data**: Place datasets in the appropriate `data/` subdirectories\n",
    "4. **Track Experiments**: Use the `experiments/` folder for model iterations\n",
    "5. **Build Reusable Code**: Add functions to the `src/` folder\n",
    "6. **Version Control**: Commit your changes to Git\n",
    "\n",
    "### üìö Learning Path Suggestions:\n",
    "\n",
    "- **Beginner**: Start with data exploration and basic sklearn models\n",
    "- **Intermediate**: Experiment with feature engineering and model evaluation\n",
    "- **Advanced**: Implement deep learning models and MLOps practices\n",
    "\n",
    "### üõ†Ô∏è Useful Commands:\n",
    "\n",
    "```bash\n",
    "# Activate environment\n",
    "ml_env\\\\Scripts\\\\activate  # Windows\n",
    "source ml_env/bin/activate  # macOS/Linux\n",
    "\n",
    "# Install packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter lab\n",
    "\n",
    "# Git commands\n",
    "git add .\n",
    "git commit -m \"Your message\"\n",
    "git push\n",
    "```\n",
    "\n",
    "Happy Machine Learning! ü§ñüìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ce7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and configure Jupyter kernel for this environment\n",
    "def setup_jupyter_kernel():\n",
    "    try:\n",
    "        # Install ipykernel if not already installed\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ipykernel'])\n",
    "        \n",
    "        # Add kernel to Jupyter\n",
    "        kernel_name = f\"ml_env_{project_root.name}\"\n",
    "        subprocess.check_call([\n",
    "            sys.executable, '-m', 'ipykernel', 'install', \n",
    "            '--user', '--name', kernel_name, '--display-name', f\"Python (ML - {project_root.name})\"\n",
    "        ])\n",
    "        \n",
    "        print(f\"‚úÖ Added Jupyter kernel: {kernel_name}\")\n",
    "        print(\"You can now select this kernel in Jupyter notebooks!\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è  Error setting up Jupyter kernel: {e}\")\n",
    "\n",
    "# Create Jupyter config directory and basic configuration\n",
    "jupyter_config = {\n",
    "    \"NotebookApp\": {\n",
    "        \"notebook_dir\": str(project_root),\n",
    "        \"open_browser\": True,\n",
    "        \"port\": 8888\n",
    "    },\n",
    "    \"InteractiveShell\": {\n",
    "        \"ast_node_interactivity\": \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß Jupyter Configuration:\")\n",
    "print(\"- Notebook directory:\", project_root)\n",
    "print(\"- Auto-open browser: True\")\n",
    "print(\"- Default port: 8888\")\n",
    "print(\"- Show all output: True\")\n",
    "\n",
    "# Set up the kernel (uncomment when ready)\n",
    "print(\"\\nüìã To set up Jupyter kernel, uncomment and run:\")\n",
    "print(\"# setup_jupyter_kernel()\")\n",
    "\n",
    "# Useful Jupyter magic commands to remember\n",
    "print(\"\\n‚ú® Useful Jupyter Magic Commands:\")\n",
    "magic_commands = [\n",
    "    \"%matplotlib inline  # Display plots inline\",\n",
    "    \"%load_ext autoreload  # Auto-reload modules\",\n",
    "    \"%autoreload 2  # Reload all modules before executing\",\n",
    "    \"%%time  # Time execution of cell\",\n",
    "    \"%debug  # Enter debugger after exception\",\n",
    "    \"%who  # List all variables\",\n",
    "    \"%whos  # List variables with details\"\n",
    "]\n",
    "\n",
    "for cmd in magic_commands:\n",
    "    print(f\"  {cmd}\")\n",
    "\n",
    "# Uncomment to set up the kernel:\n",
    "# setup_jupyter_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4470ae",
   "metadata": {},
   "source": [
    "## 7. Create Data Directories with Documentation\n",
    "\n",
    "Proper data organization is crucial for ML projects. Let's create data directories with clear documentation and naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documentation for data directories\n",
    "data_docs = {\n",
    "    'raw': \"\"\"# Raw Data Directory\n",
    "\n",
    "Store original, immutable datasets here.\n",
    "\n",
    "## Guidelines:\n",
    "- Never modify files in this directory\n",
    "- Document data sources and collection methods\n",
    "- Use descriptive filenames with dates: `sales_data_2024-01-15.csv`\n",
    "- Include metadata files: `dataset_description.txt`\n",
    "\n",
    "## Naming Convention:\n",
    "- Use lowercase with underscores: `customer_data.csv`\n",
    "- Include date stamps: `web_logs_2024_01.json`\n",
    "- Add version numbers if needed: `model_features_v2.parquet`\n",
    "\n",
    "## Data Sources:\n",
    "- Add your data source information here\n",
    "- Include URLs, APIs, or database connections\n",
    "- Note any access requirements or credentials needed\n",
    "\"\"\",\n",
    "    \n",
    "    'processed': \"\"\"# Processed Data Directory\n",
    "\n",
    "Store cleaned and preprocessed datasets here.\n",
    "\n",
    "## Guidelines:\n",
    "- Save intermediate processing steps\n",
    "- Include data validation and quality checks\n",
    "- Document transformations applied\n",
    "- Keep processing scripts in src/ directory\n",
    "\n",
    "## Naming Convention:\n",
    "- Prefix with processing stage: `01_cleaned_data.csv`\n",
    "- Include transformation type: `normalized_features.pkl`\n",
    "- Version your datasets: `final_dataset_v3.parquet`\n",
    "\n",
    "## Processing Pipeline:\n",
    "1. Raw data ‚Üí Cleaned data (remove nulls, fix types)\n",
    "2. Cleaned ‚Üí Features (feature engineering)\n",
    "3. Features ‚Üí Model-ready (scaled, encoded)\n",
    "\"\"\",\n",
    "    \n",
    "    'external': \"\"\"# External Data Directory\n",
    "\n",
    "Store external datasets and reference files here.\n",
    "\n",
    "## Guidelines:\n",
    "- Downloaded datasets from public sources\n",
    "- API responses and web scraping results\n",
    "- Reference datasets for benchmarking\n",
    "- Include licenses and attribution\n",
    "\n",
    "## Common Sources:\n",
    "- Kaggle competitions and datasets\n",
    "- UCI Machine Learning Repository\n",
    "- Government open data portals\n",
    "- Academic datasets and papers\n",
    "- Company APIs and databases\n",
    "\n",
    "## Documentation:\n",
    "- Always include source URLs\n",
    "- Note download dates\n",
    "- Include any terms of use or licenses\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Create data directories with documentation\n",
    "data_dir = project_root / \"data\"\n",
    "for subdir_name, doc_content in data_docs.items():\n",
    "    subdir = data_dir / subdir_name\n",
    "    subdir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create README file for each directory\n",
    "    readme_path = subdir / \"README.md\"\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(doc_content)\n",
    "    \n",
    "    print(f\"‚úÖ Created {subdir_name}/ directory with documentation\")\n",
    "\n",
    "# Create a data manifest template\n",
    "manifest_content = \"\"\"# Data Manifest\n",
    "\n",
    "Track all datasets used in this project.\n",
    "\n",
    "| Dataset | Source | Date Added | Size | Description | Location |\n",
    "|---------|--------|------------|------|-------------|----------|\n",
    "| Example Dataset | Kaggle | 2024-01-15 | 10MB | Customer data | data/raw/customers.csv |\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "| Column | Type | Description | Example |\n",
    "|--------|------|-------------|---------|\n",
    "| customer_id | int | Unique identifier | 12345 |\n",
    "| age | int | Customer age | 25 |\n",
    "| city | str | Customer city | New York |\n",
    "\n",
    "## Data Quality Notes\n",
    "\n",
    "- Missing values: Handle with mean imputation for numerical, mode for categorical\n",
    "- Outliers: Found in 'income' column, investigate further\n",
    "- Duplicates: None found\n",
    "- Data types: All correct after preprocessing\n",
    "\n",
    "## Processing Notes\n",
    "\n",
    "1. `customers_raw.csv` ‚Üí `customers_cleaned.csv`: Removed nulls, fixed data types\n",
    "2. `customers_cleaned.csv` ‚Üí `customers_features.csv`: Added derived features\n",
    "3. `customers_features.csv` ‚Üí `customers_final.csv`: Scaled and encoded for modeling\n",
    "\"\"\"\n",
    "\n",
    "manifest_path = data_dir / \"DATA_MANIFEST.md\"\n",
    "with open(manifest_path, 'w') as f:\n",
    "    f.write(manifest_content)\n",
    "\n",
    "print(f\"‚úÖ Created data manifest template: {manifest_path}\")\n",
    "print(\"\\nüìä Data organization complete!\")\n",
    "print(\"Remember to update the DATA_MANIFEST.md as you add new datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898149c",
   "metadata": {},
   "source": [
    "## 8. Set Up Environment Variables\n",
    "\n",
    "Environment variables help manage sensitive information like API keys, database connections, and configuration settings securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env template file\n",
    "env_template = \"\"\"# Environment Variables for ML Project\n",
    "# Copy this file to .env and fill in your actual values\n",
    "# NEVER commit .env files to version control!\n",
    "\n",
    "# Database Configuration\n",
    "DATABASE_URL=postgresql://username:password@localhost:5432/dbname\n",
    "DATABASE_HOST=localhost\n",
    "DATABASE_PORT=5432\n",
    "DATABASE_NAME=ml_project_db\n",
    "DATABASE_USER=your_username\n",
    "DATABASE_PASSWORD=your_password\n",
    "\n",
    "# API Keys\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "HUGGINGFACE_API_KEY=your_huggingface_key_here\n",
    "WANDB_API_KEY=your_wandb_key_here\n",
    "\n",
    "# Cloud Storage (AWS)\n",
    "AWS_ACCESS_KEY_ID=your_aws_access_key\n",
    "AWS_SECRET_ACCESS_KEY=your_aws_secret_key\n",
    "AWS_REGION=us-west-2\n",
    "S3_BUCKET_NAME=your_ml_data_bucket\n",
    "\n",
    "# Cloud Storage (GCP)\n",
    "GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account-key.json\n",
    "GCS_BUCKET_NAME=your_gcs_bucket\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "MLFLOW_EXPERIMENT_NAME=default\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_VERSION=v1.0.0\n",
    "MODEL_PATH=models/production/\n",
    "RANDOM_SEED=42\n",
    "\n",
    "# Development Settings\n",
    "DEBUG=True\n",
    "LOG_LEVEL=INFO\n",
    "\"\"\"\n",
    "\n",
    "# Create .env.template file\n",
    "env_template_path = project_root / \".env.template\"\n",
    "with open(env_template_path, 'w') as f:\n",
    "    f.write(env_template)\n",
    "\n",
    "print(f\"‚úÖ Created environment template: {env_template_path}\")\n",
    "\n",
    "# Create config.py for loading environment variables\n",
    "config_content = '''\"\"\"\n",
    "Configuration module for loading environment variables.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_path = Path(__file__).parent / \".env\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for ML project.\"\"\"\n",
    "    \n",
    "    # Database settings\n",
    "    DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "    DATABASE_HOST = os.getenv(\"DATABASE_HOST\", \"localhost\")\n",
    "    DATABASE_PORT = int(os.getenv(\"DATABASE_PORT\", 5432))\n",
    "    DATABASE_NAME = os.getenv(\"DATABASE_NAME\")\n",
    "    DATABASE_USER = os.getenv(\"DATABASE_USER\")\n",
    "    DATABASE_PASSWORD = os.getenv(\"DATABASE_PASSWORD\")\n",
    "    \n",
    "    # API Keys\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "    WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "    \n",
    "    # AWS Configuration\n",
    "    AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION = os.getenv(\"AWS_REGION\", \"us-west-2\")\n",
    "    S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\")\n",
    "    \n",
    "    # GCP Configuration\n",
    "    GOOGLE_APPLICATION_CREDENTIALS = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "    GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
    "    \n",
    "    # MLflow Configuration\n",
    "    MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5000\")\n",
    "    MLFLOW_EXPERIMENT_NAME = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", \"default\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    MODEL_VERSION = os.getenv(\"MODEL_VERSION\", \"v1.0.0\")\n",
    "    MODEL_PATH = os.getenv(\"MODEL_PATH\", \"models/production/\")\n",
    "    RANDOM_SEED = int(os.getenv(\"RANDOM_SEED\", 42))\n",
    "    \n",
    "    # Development Settings\n",
    "    DEBUG = os.getenv(\"DEBUG\", \"False\").lower() == \"true\"\n",
    "    LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
    "\n",
    "# Example usage:\n",
    "# from config import Config\n",
    "# print(Config.DATABASE_URL)\n",
    "'''\n",
    "\n",
    "config_path = project_root / \"src\" / \"config.py\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"‚úÖ Created configuration module: {config_path}\")\n",
    "\n",
    "# Instructions for using environment variables\n",
    "print(\"\\nüîê Environment Variables Setup Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Copy .env.template to .env: cp .env.template .env\")\n",
    "print(\"2. Edit .env file with your actual values\")\n",
    "print(\"3. Install python-dotenv: pip install python-dotenv\")\n",
    "print(\"4. Import config in your code: from src.config import Config\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Never commit .env files to version control!\")\n",
    "print(\"The .env file is already in .gitignore for your protection.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
